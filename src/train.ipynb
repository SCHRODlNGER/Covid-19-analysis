{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.7.4 64-bit",
   "display_name": "Python 3.7.4 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "ed29a1a66c6dfd6b2600586e9b0f682cb77c333ab63e105aebd65b813ec09769"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "from pylab import rcParams\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import random\n",
    "from scipy.interpolate import UnivariateSpline  \n",
    "import pickle\n",
    "from sklearn.decomposition import PCA\n",
    "import time\n",
    "import datetime\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "from pathlib import Path\n",
    "\n",
    "from model import MV_LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = Path(\"../dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from numpy import hstack\n",
    "\n",
    "# split a multivariate sequence into samples\n",
    "def split_sequences(sequences, n_steps):\n",
    "    X, y = list(), list()\n",
    "    \n",
    "    for i in range(0,len(sequences),100):\n",
    "        # find the end of this pattern\n",
    "        end_ix = i + n_steps\n",
    "        # check if we are beyond the dataset\n",
    "        if i!=0 and end_ix > len(sequences):\n",
    "            break\n",
    "        \n",
    "        sequences[i:end_ix,0]=np.insert(np.diff(sequences[i:end_ix,0]),0,0)\n",
    "        # gather input and output parts of the pattern\n",
    "        seq_x, seq_y = sequences[i:end_ix-33], sequences[end_ix-33:end_ix]\n",
    "        \n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return array(X), array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(base_path/\"modified_usa.csv\")\n",
    "\n",
    "is_nd = (df[\"State\"] == \"North Dakota\")\n",
    "\n",
    "df2 = df.copy(deep = True)\n",
    "\n",
    "is_ken = (df[\"State\"] == \"Kentucky\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array(['Alabama', 'Alaska', 'Arizona', 'Arkansas', 'California',\n",
       "       'Colorado', 'Connecticut', 'Delaware', 'District of Columbia',\n",
       "       'Florida', 'Georgia', 'Hawaii', 'Idaho', 'Illinois', 'Indiana',\n",
       "       'Iowa', 'Kansas', 'Kentucky', 'Louisiana', 'Maine', 'Maryland',\n",
       "       'Massachusetts', 'Michigan', 'Minnesota', 'Mississippi',\n",
       "       'Missouri', 'Montana', 'Nebraska', 'Nevada', 'New Hampshire',\n",
       "       'New Jersey', 'New Mexico', 'New York', 'North Carolina',\n",
       "       'North Dakota', 'Ohio', 'Oklahoma', 'Oregon', 'Pennsylvania',\n",
       "       'Puerto Rico', 'Rhode Island', 'South Carolina', 'South Dakota',\n",
       "       'Tennessee', 'Texas', 'Utah', 'Vermont', 'Virginia', 'Washington',\n",
       "       'West Virginia', 'Wisconsin', 'Wyoming'], dtype=object)"
      ]
     },
     "metadata": {},
     "execution_count": 34
    }
   ],
   "source": [
    "states = df[\"State\"].unique()\n",
    "states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df[df.State.isin(['Alabama', 'Alaska', 'American Samoa', 'Arizona', 'Arkansas',\n",
    "       'California', 'Colorado', 'Connecticut', 'Delaware',\n",
    "       'Diamond Princess', 'District of Columbia', 'Florida', 'Georgia',\n",
    "       'Grand Princess', 'Guam', 'Hawaii', 'Idaho', 'Illinois', 'Indiana',\n",
    "       'Iowa', 'Kansas', 'Louisiana', 'Maine', 'Maryland',\n",
    "       'Massachusetts', 'Michigan', 'Minnesota', 'Mississippi',\n",
    "       'Missouri', 'Montana', 'Nebraska', 'Nevada', 'New Hampshire',\n",
    "       'New Jersey', 'New Mexico', 'New York', 'North Carolina',\n",
    "       'North Dakota', 'Northern Mariana Islands', 'Ohio', 'Oklahoma',\n",
    "       'Oregon', 'Pennsylvania', 'Puerto Rico', 'Rhode Island',\n",
    "       'South Carolina', 'South Dakota', 'Tennessee', 'Texas', 'Utah',\n",
    "       'Vermont', 'Virgin Islands', 'Virginia', 'Washington',\n",
    "       'West Virginia', 'Wisconsin', 'Wyoming'])][[\"Confirmed\", \"Deaths\", \"lat\", \"long\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2=df2[(is_ken)][['Confirmed','lat','long','Deaths']]\n",
    "date=df2[(is_ken)][['Date','Confirmed']]\n",
    "\n",
    "date[\"Date\"] = pd.to_datetime(date[\"Date\"], format = '%Y%m%d', errors = 'ignore')\n",
    "date.set_index('Date', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = 4 # this is number of parallel inputs\n",
    "n_timesteps = 100 # this is number of timesteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Confirmed    0\n",
       "Deaths       0\n",
       "lat          0\n",
       "long         0\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 38
    }
   ],
   "source": [
    "data.isna().sum(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(9957, 67, 4) (9957, 33, 4)\n"
     ]
    }
   ],
   "source": [
    "X, Y = split_sequences(data.values, n_timesteps)\n",
    "print (X.shape,Y.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "alld=np.concatenate((X,Y),1)\n",
    "alld=alld.reshape(alld.shape[0]*alld.shape[1],alld.shape[2])\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(alld)\n",
    "X=[scaler.transform(x) for x in X]\n",
    "y=[scaler.transform(y) for y in Y]\n",
    "\n",
    "X=np.array(X)\n",
    "y=np.array(y)[:,:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "step :  0 loss :  7.552993338322267e-05\n"
     ]
    }
   ],
   "source": [
    "mv_net = MV_LSTM(n_features,67).cuda()\n",
    "criterion = torch.nn.MSELoss() # reduction='sum' created huge loss value\n",
    "optimizer = torch.optim.Adam(mv_net.parameters(), lr=1e-3)\n",
    "\n",
    "train_episodes = 1\n",
    "\n",
    "batch_size = 16\n",
    "losses = []\n",
    "\n",
    "mv_net.train()\n",
    "\n",
    "for t in range(train_episodes):\n",
    "    \n",
    "    for b in range(0,len(X),batch_size):\n",
    "       \n",
    "        p = np.random.permutation(len(X))\n",
    "        \n",
    "        inpt = X[p][b:b+batch_size,:,:]\n",
    "        target = y[p][b:b+batch_size,:]    \n",
    "        \n",
    "        x_batch = torch.tensor(inpt,dtype=torch.float32).cuda()    \n",
    "        y_batch = torch.tensor(target,dtype=torch.float32).cuda()\n",
    "       \n",
    "        mv_net.init_hidden(x_batch.size(0))\n",
    "        \n",
    "        output = mv_net(x_batch) \n",
    "        \n",
    "        \n",
    "        all_batch=torch.cat((x_batch[:,:,0], y_batch), 1)\n",
    "        \n",
    "        \n",
    "        loss = 1000*criterion(output.view(-1), all_batch.view(-1))  \n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()        \n",
    "        optimizer.zero_grad() \n",
    "    losses.append(loss.item())    \n",
    "    print('step : ' , t , 'loss : ' , loss.item())\n",
    "\n",
    "with open('../model/losses.pkl', 'wb') as f:  # Python 3: open(..., 'wb')\n",
    "    pickle.dump(losses, f,protocol=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(1, 37820, 4)\n",
      "[0.96509094 0.96509346 0.96509597 0.96511862 0.96512617 0.9651312\n",
      " 0.96513372 0.96513623 0.96514378 0.96515636 0.96516895 0.96518153\n",
      " 0.96519159 0.96519914 0.96520418 0.96521172 0.96522179 0.96522682\n",
      " 0.9652394  0.96524695 0.96525702 0.96525702 0.96526457 0.96527715\n",
      " 0.9652847  0.96530483 0.96530986 0.96532244 0.96532748 0.96532748\n",
      " 0.96534257 0.96534761 0.96536019 0.96537025 0.96538787 0.96539038\n",
      " 0.96538787 0.96540297 0.9654231  0.96545581 0.96548097 0.9655011\n",
      " 0.96551872 0.96553381 0.96555898 0.96558917 0.96560679 0.96562692\n",
      " 0.96563195 0.96567473 0.96567976 0.96570241 0.96574015 0.96576783\n",
      " 0.96580055 0.96580306 0.96582571 0.9658559  0.9659012  0.96598927\n",
      " 0.96603708 0.96608237 0.96610251 0.9661176  0.96614277 0.96618051\n",
      " 0.96618051] [[[0.9681258 ]\n",
      "  [0.9680949 ]\n",
      "  [0.9681351 ]\n",
      "  [0.9681682 ]\n",
      "  [0.96804404]\n",
      "  [0.9681548 ]\n",
      "  [0.9681589 ]\n",
      "  [0.9681655 ]\n",
      "  [0.9680719 ]\n",
      "  [0.9679606 ]\n",
      "  [0.9678945 ]\n",
      "  [0.9679776 ]\n",
      "  [0.96799433]\n",
      "  [0.9681494 ]\n",
      "  [0.96795684]\n",
      "  [0.9678881 ]\n",
      "  [0.9680476 ]\n",
      "  [0.96794534]\n",
      "  [0.96792597]\n",
      "  [0.968218  ]\n",
      "  [0.96706945]\n",
      "  [0.9677892 ]\n",
      "  [0.9681101 ]\n",
      "  [0.9678774 ]\n",
      "  [0.9682483 ]\n",
      "  [0.967887  ]\n",
      "  [0.96789324]\n",
      "  [0.9679845 ]\n",
      "  [0.96802396]\n",
      "  [0.9681198 ]\n",
      "  [0.96770054]\n",
      "  [0.9680714 ]\n",
      "  [0.9679614 ]\n",
      "  [0.9679117 ]\n",
      "  [0.9679634 ]\n",
      "  [0.9678982 ]\n",
      "  [0.9679432 ]\n",
      "  [0.9681761 ]\n",
      "  [0.9681123 ]\n",
      "  [0.96797234]\n",
      "  [0.96796596]\n",
      "  [0.9680748 ]\n",
      "  [0.9681092 ]\n",
      "  [0.968195  ]\n",
      "  [0.9683184 ]\n",
      "  [0.9679988 ]\n",
      "  [0.9680494 ]\n",
      "  [0.9679587 ]\n",
      "  [0.9680873 ]\n",
      "  [0.96832186]\n",
      "  [0.96795255]\n",
      "  [0.968147  ]\n",
      "  [0.96800774]\n",
      "  [0.9681722 ]\n",
      "  [0.9680632 ]\n",
      "  [0.96796393]\n",
      "  [0.9681221 ]\n",
      "  [0.9681178 ]\n",
      "  [0.96825266]\n",
      "  [0.96822417]\n",
      "  [0.9669904 ]\n",
      "  [0.96802205]\n",
      "  [0.9680328 ]\n",
      "  [0.96797556]\n",
      "  [0.96809375]\n",
      "  [0.96806985]\n",
      "  [0.9681141 ]\n",
      "  [0.96811813]\n",
      "  [0.9681309 ]\n",
      "  [0.9680844 ]\n",
      "  [0.9677256 ]\n",
      "  [0.9681921 ]\n",
      "  [0.96802986]\n",
      "  [0.9678674 ]\n",
      "  [0.9680483 ]\n",
      "  [0.96807766]\n",
      "  [0.9680099 ]\n",
      "  [0.9678879 ]\n",
      "  [0.96811736]\n",
      "  [0.96807396]\n",
      "  [0.9674423 ]\n",
      "  [0.9678334 ]\n",
      "  [0.96806127]\n",
      "  [0.96820134]\n",
      "  [0.968016  ]\n",
      "  [0.96810246]\n",
      "  [0.9681617 ]\n",
      "  [0.96796185]\n",
      "  [0.96804684]\n",
      "  [0.96812886]\n",
      "  [0.9678862 ]\n",
      "  [0.9680931 ]\n",
      "  [0.9679578 ]\n",
      "  [0.96823835]\n",
      "  [0.96816045]\n",
      "  [0.96800864]\n",
      "  [0.9683072 ]\n",
      "  [0.96803814]\n",
      "  [0.96797544]\n",
      "  [0.96790755]]]\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (100,5) (4,) (100,5) ",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-42-da5941b84808>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdata2x\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m67\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlstm_out\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[0mactual_predictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minverse_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlstm_out\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdata2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m67\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mactual_predictions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\preprocessing\\_data.py\u001b[0m in \u001b[0;36minverse_transform\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    434\u001b[0m                         force_all_finite=\"allow-nan\")\n\u001b[0;32m    435\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 436\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m-=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmin_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    437\u001b[0m         \u001b[0mX\u001b[0m \u001b[1;33m/=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscale_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    438\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (100,5) (4,) (100,5) "
     ]
    }
   ],
   "source": [
    "data2x=data2\n",
    "truth = data2\n",
    "\n",
    "data2x.values[0:len(data2x),0]=np.insert(np.diff(data2x.values[0:len(data2x),0]),0,0)\n",
    "data2x=scaler.transform(data2x) \n",
    "\n",
    "\n",
    "X_test = np.expand_dims(data2x, axis=0)\n",
    "print (X_test.shape)\n",
    "mv_net.init_hidden(1)\n",
    "\n",
    "\n",
    "lstm_out = mv_net(torch.tensor(X_test[:,-67:,:],dtype=torch.float32).cuda())\n",
    "lstm_out=lstm_out.reshape(1,100,1).cpu().data.numpy()\n",
    "\n",
    "print (data2x[-67:,0],lstm_out)\n",
    "actual_predictions = scaler.inverse_transform(np.tile(lstm_out, (1, 1,5))[0])[:,0]\n",
    "\n",
    "print (data2.values[-67:,0],actual_predictions)\n",
    "\n",
    "#actual_predictions=lstm_out\n",
    "\n",
    "\n",
    "x = np.arange(0, 54, 1)\n",
    "x2 = np.arange(0, 67, 1)\n",
    "x3 = np.arange(0, 100, 10)\n",
    "x4 = np.arange(0, 50, 1)\n",
    "\n",
    "\n",
    "#save prediction\n",
    "with open('./lstmdata/predict_indo8.pkl', 'wb') as f:  # Python 3: open(..., 'wb')\n",
    "    pickle.dump(pd.Series(actual_predictions), f,protocol=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}