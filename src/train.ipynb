{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.7.4 64-bit",
   "display_name": "Python 3.7.4 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "ed29a1a66c6dfd6b2600586e9b0f682cb77c333ab63e105aebd65b813ec09769"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "from pylab import rcParams\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import random\n",
    "from scipy.interpolate import UnivariateSpline  \n",
    "import pickle\n",
    "from sklearn.decomposition import PCA\n",
    "import time\n",
    "import datetime\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "\n",
    "from model import MV_LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = Path(\"../dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from numpy import hstack\n",
    "\n",
    "# split a multivariate sequence into samples\n",
    "def split_sequences(sequences, n_steps):\n",
    "    X, y = list(), list()\n",
    "    \n",
    "    for i in range(0,len(sequences),100):\n",
    "        # find the end of this pattern\n",
    "        end_ix = i + n_steps\n",
    "        # check if we are beyond the dataset\n",
    "        if i!=0 and end_ix > len(sequences):\n",
    "            break\n",
    "        \n",
    "        sequences[i:end_ix,0]=np.insert(np.diff(sequences[i:end_ix,0]),0,0)\n",
    "        # gather input and output parts of the pattern\n",
    "        seq_x, seq_y = sequences[i:end_ix-33], sequences[end_ix-33:end_ix]\n",
    "        \n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return array(X), array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(base_path/\"modified_usa.csv\")\n",
    "\n",
    "is_nd = (df[\"State\"] == \"North Dakota\")\n",
    "\n",
    "df2 = df.copy(deep = True)\n",
    "\n",
    "is_ken = (df[\"State\"] == \"Kentucky\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array(['Alabama', 'Alaska', 'Arizona', 'Arkansas', 'California',\n",
       "       'Colorado', 'Connecticut', 'Delaware', 'District of Columbia',\n",
       "       'Florida', 'Georgia', 'Hawaii', 'Idaho', 'Illinois', 'Indiana',\n",
       "       'Iowa', 'Kansas', 'Kentucky', 'Louisiana', 'Maine', 'Maryland',\n",
       "       'Massachusetts', 'Michigan', 'Minnesota', 'Mississippi',\n",
       "       'Missouri', 'Montana', 'Nebraska', 'Nevada', 'New Hampshire',\n",
       "       'New Jersey', 'New Mexico', 'New York', 'North Carolina',\n",
       "       'North Dakota', 'Ohio', 'Oklahoma', 'Oregon', 'Pennsylvania',\n",
       "       'Puerto Rico', 'Rhode Island', 'South Carolina', 'South Dakota',\n",
       "       'Tennessee', 'Texas', 'Utah', 'Vermont', 'Virginia', 'Washington',\n",
       "       'West Virginia', 'Wisconsin', 'Wyoming'], dtype=object)"
      ]
     },
     "metadata": {},
     "execution_count": 34
    }
   ],
   "source": [
    "states = df[\"State\"].unique()\n",
    "states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df[df.State.isin(['Alabama', 'Alaska', 'American Samoa', 'Arizona', 'Arkansas',\n",
    "       'California', 'Colorado', 'Connecticut', 'Delaware',\n",
    "       'Diamond Princess', 'District of Columbia', 'Florida', 'Georgia',\n",
    "       'Grand Princess', 'Guam', 'Hawaii', 'Idaho', 'Illinois', 'Indiana',\n",
    "       'Iowa', 'Kansas', 'Louisiana', 'Maine', 'Maryland',\n",
    "       'Massachusetts', 'Michigan', 'Minnesota', 'Mississippi',\n",
    "       'Missouri', 'Montana', 'Nebraska', 'Nevada', 'New Hampshire',\n",
    "       'New Jersey', 'New Mexico', 'New York', 'North Carolina',\n",
    "       'North Dakota', 'Northern Mariana Islands', 'Ohio', 'Oklahoma',\n",
    "       'Oregon', 'Pennsylvania', 'Puerto Rico', 'Rhode Island',\n",
    "       'South Carolina', 'South Dakota', 'Tennessee', 'Texas', 'Utah',\n",
    "       'Vermont', 'Virgin Islands', 'Virginia', 'Washington',\n",
    "       'West Virginia', 'Wisconsin', 'Wyoming'])][[\"Confirmed\", \"Deaths\", \"lat\", \"long\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2=df2[(is_ken)][['Confirmed','lat','long','Deaths']]\n",
    "date=df2[(is_ken)][['Date','Confirmed']]\n",
    "\n",
    "date[\"Date\"] = pd.to_datetime(date[\"Date\"], format = '%Y%m%d', errors = 'ignore')\n",
    "date.set_index('Date', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = 4 # this is number of parallel inputs\n",
    "n_timesteps = 100 # this is number of timesteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Confirmed    0\n",
       "Deaths       0\n",
       "lat          0\n",
       "long         0\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 38
    }
   ],
   "source": [
    "data.isna().sum(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(9957, 67, 4) (9957, 33, 4)\n"
     ]
    }
   ],
   "source": [
    "X, Y = split_sequences(data.values, n_timesteps)\n",
    "print (X.shape,Y.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "alld=np.concatenate((X,Y),1)\n",
    "alld=alld.reshape(alld.shape[0]*alld.shape[1],alld.shape[2])\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(alld)\n",
    "X=[scaler.transform(x) for x in X]\n",
    "y=[scaler.transform(y) for y in Y]\n",
    "\n",
    "X=np.array(X)\n",
    "y=np.array(y)[:,:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "step :  0 loss :  7.552993338322267e-05\n"
     ]
    }
   ],
   "source": [
    "mv_net = MV_LSTM(n_features,67).cuda()\n",
    "criterion = torch.nn.MSELoss() # reduction='sum' created huge loss value\n",
    "optimizer = torch.optim.Adam(mv_net.parameters(), lr=1e-3)\n",
    "\n",
    "train_episodes = 10000\n",
    "\n",
    "batch_size = 16\n",
    "losses = []\n",
    "\n",
    "mv_net.train()\n",
    "\n",
    "for t in range(train_episodes):\n",
    "    \n",
    "    for b in range(0,len(X),batch_size):\n",
    "       \n",
    "        p = np.random.permutation(len(X))\n",
    "        \n",
    "        inpt = X[p][b:b+batch_size,:,:]\n",
    "        target = y[p][b:b+batch_size,:]    \n",
    "        \n",
    "        x_batch = torch.tensor(inpt,dtype=torch.float32).cuda()    \n",
    "        y_batch = torch.tensor(target,dtype=torch.float32).cuda()\n",
    "       \n",
    "        mv_net.init_hidden(x_batch.size(0))\n",
    "        \n",
    "        output = mv_net(x_batch) \n",
    "        \n",
    "        \n",
    "        all_batch=torch.cat((x_batch[:,:,0], y_batch), 1)\n",
    "        \n",
    "        \n",
    "        loss = 1000*criterion(output.view(-1), all_batch.view(-1))  \n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()        \n",
    "        optimizer.zero_grad() \n",
    "    losses.append(loss.item())    \n",
    "    print('step : ' , t , 'loss : ' , loss.item())\n",
    "\n",
    "with open('../model/losses.pkl', 'wb') as f:  # Python 3: open(..., 'wb')\n",
    "    pickle.dump(losses, f,protocol=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2x=data2\n",
    "truth = data2\n",
    "\n",
    "data2x.values[0:len(data2x),0]=np.insert(np.diff(data2x.values[0:len(data2x),0]),0,0)\n",
    "data2x=scaler.transform(data2x) \n",
    "\n",
    "\n",
    "X_test = np.expand_dims(data2x, axis=0)\n",
    "# print (X_test.shape)\n",
    "mv_net.init_hidden(1)\n",
    "\n",
    "\n",
    "lstm_out = mv_net(torch.tensor(X_test[:,-67:,:],dtype=torch.float32).cuda())\n",
    "lstm_out=lstm_out.reshape(1,100,1).cpu().data.numpy()\n",
    "\n",
    "# print (data2x[-67:,0],lstm_out)\n",
    "actual_predictions = scaler.inverse_transform(np.tile(lstm_out, (1, 1,4))[0])[:,0]\n",
    "\n",
    "\n",
    "#actual_predictions=lstm_out\n",
    "\n",
    "\n",
    "x = np.arange(0, 54, 1)\n",
    "x2 = np.arange(0, 67, 1)\n",
    "x3 = np.arange(0, 100, 10)\n",
    "x4 = np.arange(0, 50, 1)\n",
    "\n",
    "\n",
    "#save prediction\n",
    "with open('../predictions/predict_ken.pkl', 'wb') as f:  # Python 3: open(..., 'wb')\n",
    "    pickle.dump(pd.Series(actual_predictions), f,protocol=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Index(['2020-01-22', '2020-01-23', '2020-01-24', '2020-01-25', '2020-01-26',\n       '2020-01-27', '2020-01-28', '2020-01-29', '2020-01-30', '2020-01-31',\n       ...\n       '2020-11-17', '2020-11-18', '2020-11-19', '2020-11-20', '2020-11-21',\n       '2020-11-22', '2020-11-23', '2020-11-24', '2020-11-25', '2020-11-26'],\n      dtype='object', name='Date', length=37820)\nDatetimeIndex(['2020-01-22', '2020-01-23', '2020-01-24', '2020-01-25',\n               '2020-01-26', '2020-01-27', '2020-01-28', '2020-01-29',\n               '2020-01-30', '2020-01-31',\n               ...\n               '2020-11-17', '2020-11-18', '2020-11-19', '2020-11-20',\n               '2020-11-21', '2020-11-22', '2020-11-23', '2020-11-24',\n               '2020-11-25', '2020-11-26'],\n              dtype='datetime64[ns]', length=310, freq='D')\n(37820,) (100,)\n(37820, 1)\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [37820, 100]",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-58-c5a4329545cc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mConfirmed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msumpred\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmean_squared_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mConfirmed\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msumpred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m67\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcumsum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfirmed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m67\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdate_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msumpred\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Prediction'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\metrics\\_regression.py\u001b[0m in \u001b[0;36mmean_squared_error\u001b[1;34m(y_true, y_pred, sample_weight, multioutput, squared)\u001b[0m\n\u001b[0;32m    250\u001b[0m     \"\"\"\n\u001b[0;32m    251\u001b[0m     y_type, y_true, y_pred, multioutput = _check_reg_targets(\n\u001b[1;32m--> 252\u001b[1;33m         y_true, y_pred, multioutput)\n\u001b[0m\u001b[0;32m    253\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    254\u001b[0m     output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\metrics\\_regression.py\u001b[0m in \u001b[0;36m_check_reg_targets\u001b[1;34m(y_true, y_pred, multioutput, dtype)\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m     \"\"\"\n\u001b[1;32m---> 84\u001b[1;33m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     85\u001b[0m     \u001b[0my_true\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    210\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    211\u001b[0m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[1;32m--> 212\u001b[1;33m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0m\u001b[0;32m    213\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [37820, 100]"
     ]
    }
   ],
   "source": [
    "fig, ax = plt.subplots() \n",
    "plt.title('Days vs Confirmed Cases Accumulation')\n",
    "plt.ylabel('Confirmed')\n",
    "\n",
    "left, width = .25, .5\n",
    "bottom, height = .25, .5\n",
    "right = left + width\n",
    "top = bottom + height\n",
    "\n",
    "print (date.index)\n",
    "date_list=pd.date_range(start=date.index[0],end=date.index[-1])\n",
    "print (date_list)\n",
    "\n",
    "plt.axvline(x=np.array(date_list)[66], color='r', linestyle='--')\n",
    "\n",
    "ax.text(0.2*(left+right), 0.8*(bottom+top), 'input sequence',\n",
    "        horizontalalignment='left',\n",
    "        verticalalignment='center',\n",
    "        fontsize=10, color='red',\n",
    "        transform=ax.transAxes)\n",
    "ax.text(0.0125*(left+right), 0.77*(bottom+top), '______________________',\n",
    "        horizontalalignment='left',\n",
    "        verticalalignment='center',\n",
    "        fontsize=20, color='red',\n",
    "        transform=ax.transAxes)\n",
    "\n",
    "\n",
    "\n",
    "sumpred=np.cumsum(np.absolute(actual_predictions))\n",
    "\n",
    "print(date.Confirmed.shape, sumpred.shape)\n",
    "print (date.values.shape) \n",
    "print (sqrt(mean_squared_error(date.Confirmed,sumpred)))          \n",
    "plt.plot(date.values[-67:],np.cumsum(data2.confirmed.values[-67:]))\n",
    "plt.plot(np.array(date_list),sumpred,label='Prediction')\n",
    "plt.plot(np.array(date_list),date.confirmed,label='Actual')\n",
    "plt.xticks(rotation=90)\n",
    "fig.autofmt_xdate()\n",
    "plt.legend(loc=2)\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}